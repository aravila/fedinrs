{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for FedLab users\n",
    "\n",
    "This is a comprehensive tutorial for users who would like to know FedLab. FedLab is built on the top of [torch.distributed](torch.distributed) modules and provides the necessary modules for FL simulation, including communication, compression, model optimization, data partition, and other functional modules. FedLab users can build FL simulation environment with custom modules like playing with LEGO bricks. \n",
    "\n",
    "In this tutorial, we will further describe the architecture of FedLab and its usage. To put it simply, we introduce FedLab by implementing a vanilla federated learning algorithm FedAvg in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# configuration\n",
    "from munch import Munch\n",
    "from fedlab.models.mlp import MLP\n",
    "\n",
    "model = MLP(784, 10)\n",
    "args = Munch\n",
    "\n",
    "args.total_client = 100\n",
    "args.alpha = 0.5\n",
    "args.seed = 42\n",
    "args.preprocess = True\n",
    "args.cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare your dataset\n",
    "\n",
    "FedLab provide necessary module for uses to patition their datasets. Additionally, various implementation of datasets partition for federated learning are also availiable at the [URL](https://github.com/SMILELab-FL/FedLab/tree/master/fedlab/dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a example usage of patitioned MNIST dataset\n",
    "# Download raw MNIST dataset and partition them according to given configuration\n",
    "\n",
    "from torchvision import transforms\n",
    "from fedlab.contrib.dataset.partitioned_mnist import PartitionedMNIST\n",
    "\n",
    "fed_mnist = PartitionedMNIST(root=\"../datasets/mnist/\",\n",
    "                         path=\"../datasets/mnist/fedmnist/\",\n",
    "                         num_clients=args.total_client,\n",
    "                         partition=\"noniid-labeldir\",\n",
    "                         dir_alpha=args.alpha,\n",
    "                         seed=args.seed,\n",
    "                         preprocess=args.preprocess,\n",
    "                         download=True,\n",
    "                         verbose=True,\n",
    "                         transform=transforms.Compose(\n",
    "                             [transforms.ToPILImage(), transforms.ToTensor()]))\n",
    "\n",
    "dataset = fed_mnist.get_dataset(0) # get the 0-th client's dataset\n",
    "dataloader = fed_mnist.get_dataloader(0, batch_size=128) # get the 0-th client's dataset loader with batch size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define client local training\n",
    "\n",
    "Client training procedure is implemented by class ClientTrainer in FedLab. We have built-in FedAvg implementation in FedLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client\n",
    "from fedlab.contrib.algorithm.basic_client import SGDSerialClientTrainer, SGDClientTrainer\n",
    "\n",
    "# local train configuration\n",
    "args.epochs = 5\n",
    "args.batch_size = 128\n",
    "args.lr = 0.1\n",
    "\n",
    "trainer = SGDSerialClientTrainer(model, args.total_client, cuda=args.cuda) # serial trainer\n",
    "# trainer = SGDClientTrainer(model, cuda=True) # single trainer\n",
    "\n",
    "trainer.setup_dataset(fed_mnist)\n",
    "trainer.setup_optim(args.epochs, args.batch_size, args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define server global aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server\n",
    "from fedlab.contrib.algorithm.basic_server import SyncServerHandler\n",
    "\n",
    "# global configuration\n",
    "args.com_round = 10\n",
    "args.sample_ratio = 0.1\n",
    "\n",
    "handler = SyncServerHandler(model=model, global_round=args.com_round, sample_ratio=args.sample_ratio, cuda=args.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choose simulation mode and run\n",
    "\n",
    "We provide three basic simulation mode in FedLab. Depending on the needs of users.\n",
    "\n",
    "1. Choose Standalone mode to run the simulation with lowest resourch allocation.\n",
    "2. Choose Cross-process mode to run the simulation with multi-machines or multi-gpus with faster calculation.\n",
    "3. Chosse Hierachical mode to run the simulation across computer clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone\n",
    "\n",
    "We provide an example pipeline implementation. Please see [URL](https://github.com/SMILELab-FL/FedLab/blob/master/fedlab/core/standalone.py).\n",
    "\n",
    "If you change the data partition paramters $\\alpha$, you could get following convergence curves, which reavels the Non-IID challenge in federated learning.\n",
    "\n",
    "![](./examples/imgs/non_iid_impacts_on_fedavg.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 22.4721, accuracy 0.2045\n",
      "loss 24.2379, accuracy 0.0982\n",
      "loss 23.1788, accuracy 0.1490\n",
      "loss 21.5608, accuracy 0.1632\n",
      "loss 19.1702, accuracy 0.3320\n",
      "loss 17.8418, accuracy 0.3595\n",
      "loss 15.6652, accuracy 0.5358\n",
      "loss 13.2652, accuracy 0.5735\n",
      "loss 13.1869, accuracy 0.5670\n",
      "loss 11.1327, accuracy 0.6535\n"
     ]
    }
   ],
   "source": [
    "from fedlab.utils.functional import evaluate\n",
    "from fedlab.core.standalone import StandalonePipeline\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "class EvalPipeline(StandalonePipeline):\n",
    "    def __init__(self, handler, trainer, test_loader):\n",
    "        super().__init__(handler, trainer)\n",
    "        self.test_loader = test_loader \n",
    "    \n",
    "    def main(self):\n",
    "        while self.handler.if_stop is False:\n",
    "            # server side\n",
    "            sampled_clients = self.handler.sample_clients()\n",
    "            broadcast = self.handler.downlink_package\n",
    "            \n",
    "            # client side\n",
    "            self.trainer.local_process(broadcast, sampled_clients)\n",
    "            uploads = self.trainer.uplink_package\n",
    "\n",
    "            # server side\n",
    "            for pack in uploads:\n",
    "                self.handler.load(pack)\n",
    "\n",
    "            loss, acc = evaluate(self.handler.model, nn.CrossEntropyLoss(), self.test_loader)\n",
    "            print(\"loss {:.4f}, test accuracy {:.4f}\".format(loss, acc))\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root=\"../datasets/mnist/\",\n",
    "                                       train=False,\n",
    "                                       transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_data, batch_size=1024)\n",
    "\n",
    "standalone_eval = EvalPipeline(handler=handler, trainer=trainer, test_loader=test_loader)\n",
    "standalone_eval.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-process\n",
    "\n",
    "Due to the jupyter doesn't support multi-process program, we provide only the description of Cross-process mode and Hierachical mode in this part.\n",
    "\n",
    "For runable scripts, please see our examples in [cross-process](https://github.com/SMILELab-FL/FedLab/tree/master/examples/cross-process-mnist) and [scale](https://github.com/SMILELab-FL/FedLab/tree/master/examples/scale-mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedlab.core import DistNetwork\n",
    "from fedlab.core.client.manager import PassiveClientManager\n",
    "\n",
    "# Client side. Put your trainer into a network manager.\n",
    "\n",
    "args.ip = \"127.0.0.1\"\n",
    "args.port = 3002\n",
    "args.rank = 1\n",
    "args.world_size = 2 # world_size = the number of client manager + 1 (server)\n",
    "\n",
    "args.ethernet = None\n",
    "\n",
    "client_network = DistNetwork(\n",
    "    address=(args.ip, args.port),\n",
    "    world_size=args.world_size,\n",
    "    rank=args.rank,\n",
    "    ethernet=args.ethernet,\n",
    ")\n",
    "\n",
    "# trainer can be ordinary trainer or serial trainer.\n",
    "client_manager = PassiveClientManager(trainer=trainer,\n",
    "                                network=client_network)\n",
    "\n",
    "# Server side. Put your handler into a network manager.\n",
    "from fedlab.core.server import SynchronousServerManager\n",
    "\n",
    "server_network = DistNetwork(address=(args.ip, args.port),\n",
    "                      world_size=args.world_size,\n",
    "                      rank=0, # the rank of server is 0 as default\n",
    "                      ethernet=args.ethernet)\n",
    "\n",
    "server_manager = SynchronousServerManager(handler=handler,\n",
    "                                    network=server_network,\n",
    "                                    mode=\"GLOBAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierachical\n",
    "\n",
    "Hierachical is a complex network topology implementation in FedLab. Hierarchical mode for FedLab is designed for situation tasks on multiple computer clusters (in different LAN) or the real-world scenes. To enable the inter-connection for different computer clusters, FedLab develops Scheduler as middle-server process to connect client groups. Each Scheduler manages the communication between the global server and clients in a client group. And server can communicate with clients in different LAN via corresponding Scheduler. The computation mode of a client group for each scheduler can be either standalone or cross-process.\n",
    "\n",
    "A hierarchical FL system with K client groups is depicted as below. For a runable scripts, please see [here](https://github.com/SMILELab-FL/FedLab/tree/master/examples/hierarchical-hybrid-mnist)\n",
    "\n",
    "![](./docs/imgs/fedlab-hierarchical.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('fedlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019ae50596e3d4df627f3288be8543f4b17347150bdb9d2aa2e7c637014aee00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
